{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Politikbereich</th>\n",
       "      <th>Zweck</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Verkehr</td>\n",
       "      <td>Nord-Süd-Tangente; Linie 26/27, 2.2. Teil-BPU,A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bildung</td>\n",
       "      <td>Gedenken zu 30 Jahre Mauerfall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Stadtentwicklung</td>\n",
       "      <td>Lernen Na Logo - Bildungsnetzwerk Hellersdorfer Promenade</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jugend</td>\n",
       "      <td>Kinder- und Jugendambulanz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jugend</td>\n",
       "      <td>Therapiebad</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Politikbereich                                                      Zweck\n",
       "0           Verkehr            Nord-Süd-Tangente; Linie 26/27, 2.2. Teil-BPU,A\n",
       "1           Bildung                             Gedenken zu 30 Jahre Mauerfall\n",
       "2  Stadtentwicklung  Lernen Na Logo - Bildungsnetzwerk Hellersdorfer Promenade\n",
       "3            Jugend                                 Kinder- und Jugendambulanz\n",
       "4            Jugend                                                Therapiebad"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"data/preprocessed_data.csv\")\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df.loc[:, df.columns != 'Politikbereich'], df.loc[:,df.columns == 'Politikbereich']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([21,  2, 18, ..., 20, 20, 20])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "le.fit(y[\"Politikbereich\"].unique().tolist())\n",
    "y = y[\"Politikbereich\"].apply(lambda s: le.transform([s])[0]).values\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce RTX 2070 with Max-Q Design\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-german-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>758</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.680419</td>\n",
       "      <td>0.433664</td>\n",
       "      <td>0.439248</td>\n",
       "      <td>0.866302</td>\n",
       "      <td>-0.352283</td>\n",
       "      <td>0.405156</td>\n",
       "      <td>0.042996</td>\n",
       "      <td>0.474451</td>\n",
       "      <td>-0.042971</td>\n",
       "      <td>-0.087204</td>\n",
       "      <td>...</td>\n",
       "      <td>0.303186</td>\n",
       "      <td>-0.299498</td>\n",
       "      <td>0.331566</td>\n",
       "      <td>0.211172</td>\n",
       "      <td>0.274229</td>\n",
       "      <td>0.532789</td>\n",
       "      <td>-0.644731</td>\n",
       "      <td>0.511696</td>\n",
       "      <td>0.032847</td>\n",
       "      <td>0.031498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.066066</td>\n",
       "      <td>-0.440251</td>\n",
       "      <td>-0.327377</td>\n",
       "      <td>0.168775</td>\n",
       "      <td>-0.177032</td>\n",
       "      <td>0.491001</td>\n",
       "      <td>-0.455673</td>\n",
       "      <td>0.249439</td>\n",
       "      <td>-0.828507</td>\n",
       "      <td>0.019433</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.320476</td>\n",
       "      <td>-0.197653</td>\n",
       "      <td>0.019215</td>\n",
       "      <td>-0.156263</td>\n",
       "      <td>-0.743738</td>\n",
       "      <td>0.104028</td>\n",
       "      <td>-0.065598</td>\n",
       "      <td>0.303763</td>\n",
       "      <td>0.365776</td>\n",
       "      <td>0.199047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.026205</td>\n",
       "      <td>-0.194637</td>\n",
       "      <td>0.063422</td>\n",
       "      <td>0.138446</td>\n",
       "      <td>-0.160203</td>\n",
       "      <td>0.162052</td>\n",
       "      <td>-0.022508</td>\n",
       "      <td>0.377391</td>\n",
       "      <td>-0.206134</td>\n",
       "      <td>0.017674</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.227518</td>\n",
       "      <td>0.198408</td>\n",
       "      <td>0.382017</td>\n",
       "      <td>-0.227325</td>\n",
       "      <td>0.116455</td>\n",
       "      <td>0.193815</td>\n",
       "      <td>-0.478202</td>\n",
       "      <td>0.141823</td>\n",
       "      <td>0.456079</td>\n",
       "      <td>-0.318012</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 768 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0  0.680419  0.433664  0.439248  0.866302 -0.352283  0.405156  0.042996   \n",
       "1  0.066066 -0.440251 -0.327377  0.168775 -0.177032  0.491001 -0.455673   \n",
       "2  0.026205 -0.194637  0.063422  0.138446 -0.160203  0.162052 -0.022508   \n",
       "\n",
       "        7         8         9    ...       758       759       760       761  \\\n",
       "0  0.474451 -0.042971 -0.087204  ...  0.303186 -0.299498  0.331566  0.211172   \n",
       "1  0.249439 -0.828507  0.019433  ... -0.320476 -0.197653  0.019215 -0.156263   \n",
       "2  0.377391 -0.206134  0.017674  ... -0.227518  0.198408  0.382017 -0.227325   \n",
       "\n",
       "        762       763       764       765       766       767  \n",
       "0  0.274229  0.532789 -0.644731  0.511696  0.032847  0.031498  \n",
       "1 -0.743738  0.104028 -0.065598  0.303763  0.365776  0.199047  \n",
       "2  0.116455  0.193815 -0.478202  0.141823  0.456079 -0.318012  \n",
       "\n",
       "[3 rows x 768 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "import spacy\n",
    "import torch\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from lib.bert_pytorch.helper_functions import get_device\n",
    "import nlpaug.augmenter.word as naw\n",
    "\n",
    "class SelectFeatures(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, features_list):\n",
    "        self.features_list = features_list\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        X = X[self.features_list]\n",
    "        X = X.iloc[:,0]\n",
    "        return X\n",
    "\n",
    "class CleanText(BaseEstimator, TransformerMixin):\n",
    "    def cleaner(self, text):\n",
    "        # Remove mid slash and digits\n",
    "        text = re.sub(r'-', ' ', text)\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "        # Custom ones not supported by spacy\n",
    "        text = re.sub(r'Abs\\.', 'Absatz', text)\n",
    "        text = re.sub(r'e\\.V\\.', 'eingetragener Verein', text)\n",
    "        text = re.sub(r'co\\.', 'Kompanie', text)\n",
    "        text = re.sub(r'Co\\.', 'Kompanie', text)\n",
    "        text = re.sub(r'gem\\.', 'gemäß', text)\n",
    "        text = re.sub(r\"'s\", '', text)\n",
    "        return text\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        X = X.apply(self.cleaner)\n",
    "        return X\n",
    "\n",
    "class SpacyLemmatizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.nlp = spacy.load(\"de_core_news_lg\")\n",
    "        self.nlp.remove_pipe(\"ner\")\n",
    "        self.nlp.remove_pipe(\"parser\")\n",
    "        self.nlp.remove_pipe(\"attribute_ruler\")\n",
    "    def normalize(self, text):\n",
    "        doc = self.nlp(text)\n",
    "        output = []\n",
    "        for token in doc:\n",
    "            if not token.is_punct and not token.is_stop and not token.is_space:\n",
    "                output.append(token.lemma_)\n",
    "        return \" \".join(output)\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        X = X.apply(self.normalize)\n",
    "        return X\n",
    "\n",
    "class Lowercase(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        X = X.apply(lambda text: text.lower())\n",
    "        return X\n",
    "\n",
    "class TfIdfVectorizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "    def fit(self, X, y=None):\n",
    "        self.vectorizer.fit(X)\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        tfidf_encodings = self.vectorizer.transform(X)\n",
    "        X = pd.DataFrame(tfidf_encodings.toarray())\n",
    "        return X\n",
    "\n",
    "class BertSentenceEmbeddigs(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-german-cased')\n",
    "        self.device = get_device()\n",
    "        self.model = BertModel.from_pretrained('bert-base-german-cased', \n",
    "                                                output_hidden_states=True)\\\n",
    "                                                    .to(self.device)\n",
    "        self.model.eval()\n",
    "    def embed_sentence(self, sentence: str):\n",
    "        ids_tensor = self.tokenizer.encode(sentence, return_tensors='pt')\n",
    "        ids_tensor = ids_tensor.to(self.device)\n",
    "        with torch.no_grad():\n",
    "            out = self.model(input_ids=ids_tensor)\n",
    "        hidden_states = out.hidden_states\n",
    "        last_four_layers = [hidden_states[i] for i in (-1, -2, -3, -4)]\n",
    "        sentence_embedding = torch.cat(tuple(last_four_layers), dim=0)\n",
    "        sentence_embedding = torch.mean(sentence_embedding, dim=0)\n",
    "        sentence_embedding = torch.mean(sentence_embedding, dim=0)\n",
    "        return sentence_embedding.cpu().numpy()\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        X = X.apply(self.embed_sentence)\n",
    "        X = pd.DataFrame(X.values.tolist())\n",
    "        return X\n",
    "\n",
    "prep_pipeline = Pipeline([\n",
    "    (\"select_features\", SelectFeatures(features_list=[\"Zweck\"])),\n",
    "    # (\"clean_text\", CleanText()),\n",
    "    # (\"spacy_lemmatizer\", SpacyLemmatizer()),\n",
    "    # (\"text_lowercase\", Lowercase()),\n",
    "    # (\"tfidf_vectorizer\", TfIdfVectorizer()),\n",
    "    (\"bert_sentence_embeddings\", BertSentenceEmbeddigs()),\n",
    "])\n",
    "\n",
    "# Example\n",
    "prep_pipeline.fit_transform(X, y).head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters hypertuning and model selection with GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce RTX 2070 with Max-Q Design\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-german-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce RTX 2070 with Max-Q Design\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-german-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce RTX 2070 with Max-Q Design\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-german-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce RTX 2070 with Max-Q Design\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-german-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce RTX 2070 with Max-Q Design\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-german-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce RTX 2070 with Max-Q Design\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-german-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce RTX 2070 with Max-Q Design\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-german-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce RTX 2070 with Max-Q Design\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-german-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce RTX 2070 with Max-Q Design\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-german-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce RTX 2070 with Max-Q Design\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-german-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce RTX 2070 with Max-Q Design\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-german-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "\n",
    "\n",
    "def custom_scorer_macro_f1(y_true,y_pred):\n",
    "    macro_f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    return macro_f1\n",
    "\n",
    "scorer_macro_f1 = make_scorer(custom_scorer_macro_f1, greater_is_better=True)\n",
    "\n",
    "def custom_scorer_weighted_f1(y_true,y_pred):\n",
    "    weighted_f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    return weighted_f1\n",
    "\n",
    "scorer_weighted_f1 = make_scorer(custom_scorer_weighted_f1, greater_is_better=True)\n",
    "\n",
    "\n",
    "def execute_pipeline(features,labels, search_space=[\n",
    "                    {\"estimator\": [RandomForestClassifier(random_state=42, verbose=1)],\n",
    "                    \"estimator__n_estimators\": [100],\n",
    "                    # \"estimator__max_depth\": [2, 6]\n",
    "                    }], \n",
    "                    cv = 10,\n",
    "                    verbose = 1,\n",
    "                    n_jobs = os.cpu_count() - 2,\n",
    "                    scoring = scorer_macro_f1):\n",
    "                    # scoring = {\"macro_f1\": scorer_macro_f1,\"weighted_f1\": scorer_weighted_f1}):\n",
    "    \n",
    "    pipe = Pipeline([(\"preprocessing\", prep_pipeline),\n",
    "                    (\"estimator\", RandomForestClassifier())])\n",
    "    \n",
    "    gridsearch = GridSearchCV(pipe, search_space, scoring=scoring, cv=cv, verbose=verbose,n_jobs=n_jobs,refit=True)\n",
    "    best_model = gridsearch.fit(features, labels)\n",
    "    print(best_model.best_params_)\n",
    "    print(best_model.best_score_)\n",
    "    return best_model\n",
    "\n",
    "best_estimator = execute_pipeline(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(best_estimator,open(\"pretrained_models/random_forest/best_estimator.pkl\", \"wb\" ))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9ffdfb0697f174c80f5cf0b5f22ddc30cefc12684f1f3af5faba8742076bf488"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
