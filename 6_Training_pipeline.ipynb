{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do: use augmented dataset.\n",
    "- Pros: classes balanced\n",
    "- Cons: larger vocabulary (if using Tf-idf lots of RAM is needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Politikbereich</th>\n",
       "      <th>Zweck</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Verkehr</td>\n",
       "      <td>Nord-Süd-Tangente; Linie 26/27, 2.2. Teil-BPU,A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bildung</td>\n",
       "      <td>Gedenken zu 30 Jahre Mauerfall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Stadtentwicklung</td>\n",
       "      <td>Lernen Na Logo - Bildungsnetzwerk Hellersdorfer Promenade</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jugend</td>\n",
       "      <td>Kinder- und Jugendambulanz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jugend</td>\n",
       "      <td>Therapiebad</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Politikbereich                                                      Zweck\n",
       "0           Verkehr            Nord-Süd-Tangente; Linie 26/27, 2.2. Teil-BPU,A\n",
       "1           Bildung                             Gedenken zu 30 Jahre Mauerfall\n",
       "2  Stadtentwicklung  Lernen Na Logo - Bildungsnetzwerk Hellersdorfer Promenade\n",
       "3            Jugend                                 Kinder- und Jugendambulanz\n",
       "4            Jugend                                                Therapiebad"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"data/selected_data.csv\")\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df.loc[:, df.columns != 'Politikbereich'], df.loc[:,df.columns == 'Politikbereich']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode target labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([21,  2, 18, ..., 11, 11, 18])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "classes = ['Verkehr',\n",
    "            'Bildung',\n",
    "            'Stadtentwicklung',\n",
    "            'Jugend',\n",
    "            'Integration',\n",
    "            'Wirtschaft',\n",
    "            'Familie',\n",
    "            'Kultur',\n",
    "            'Arbeit',\n",
    "            'Denkmalschutz',\n",
    "            'Soziales',\n",
    "            'Gesundheit',\n",
    "            'Gleichstellung',\n",
    "            'Wissenschaft',\n",
    "            'Sport',\n",
    "            'Bürgerschaftliches Engagement, Bürgerbeteiligung',\n",
    "            'Kirchen, Religions-, Weltanschauungsgemeinschaften',\n",
    "            'Umwelt',\n",
    "            'Antidiskriminierung',\n",
    "            'Frauen',\n",
    "            'Forschung',\n",
    "            'Verbraucherschutz',\n",
    "            'Pflege',\n",
    "            'Sicherheit, Ordnung']\n",
    "\n",
    "le.fit(classes)\n",
    "y = y[\"Politikbereich\"].apply(lambda s: le.transform([s])[0]).values\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1910</th>\n",
       "      <th>1911</th>\n",
       "      <th>1912</th>\n",
       "      <th>1913</th>\n",
       "      <th>1914</th>\n",
       "      <th>1915</th>\n",
       "      <th>1916</th>\n",
       "      <th>1917</th>\n",
       "      <th>1918</th>\n",
       "      <th>1919</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 1920 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0     1     2     3     4     5     6     7     8     9     ...  1910  \\\n",
       "0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "1   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "2   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "\n",
       "   1911  1912  1913  1914  1915  1916  1917  1918  1919  \n",
       "0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "1   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "2   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "\n",
       "[3 rows x 1920 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "import spacy\n",
    "import torch\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from lib.bert_pytorch.helper_functions import get_device\n",
    "\n",
    "class SelectFeatures(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, features_list):\n",
    "        self.features_list = features_list\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        X = X[self.features_list]\n",
    "        X = X.iloc[:,0]\n",
    "        return X\n",
    "\n",
    "class CleanText(BaseEstimator, TransformerMixin):\n",
    "    def cleaner(self, text):\n",
    "        # Remove mid slash and digits\n",
    "        text = re.sub(r'-', ' ', text)\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "        # Custom ones not supported by spacy\n",
    "        text = re.sub(r'Abs\\.', 'Absatz', text)\n",
    "        text = re.sub(r'e\\.V\\.', 'eingetragener Verein', text)\n",
    "        text = re.sub(r'co\\.', 'Kompanie', text)\n",
    "        text = re.sub(r'Co\\.', 'Kompanie', text)\n",
    "        text = re.sub(r'gem\\.', 'gemäß', text)\n",
    "        text = re.sub(r\"'s\", '', text)\n",
    "        return text\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        X = X.apply(self.cleaner)\n",
    "        return X\n",
    "\n",
    "class SpacyLemmatizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.nlp = spacy.load(\"de_core_news_lg\")\n",
    "        self.nlp.remove_pipe(\"ner\")\n",
    "        self.nlp.remove_pipe(\"parser\")\n",
    "        self.nlp.remove_pipe(\"attribute_ruler\")\n",
    "    def normalize(self, text):\n",
    "        doc = self.nlp(text)\n",
    "        output = []\n",
    "        for token in doc:\n",
    "            if not token.is_punct and not token.is_stop and not token.is_space:\n",
    "                output.append(token.lemma_)\n",
    "        return \" \".join(output)\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        X = X.apply(self.normalize)\n",
    "        return X\n",
    "\n",
    "class Lowercase(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        X = X.apply(lambda text: text.lower())\n",
    "        return X\n",
    "\n",
    "class TfIdfVectorizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "    def fit(self, X, y=None):\n",
    "        self.vectorizer.fit(X)\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        tfidf_encodings = self.vectorizer.transform(X)\n",
    "        X = pd.DataFrame(tfidf_encodings.toarray())\n",
    "        return X\n",
    "\n",
    "class BertSentenceEmbeddigs(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-german-cased')\n",
    "        self.device = get_device()\n",
    "        self.model = BertModel.from_pretrained('bert-base-german-cased', \n",
    "                                                output_hidden_states=True)\\\n",
    "                                                    .to(self.device)\n",
    "        self.model.eval()\n",
    "    def embed_sentence(self, sentence: str):\n",
    "        ids_tensor = self.tokenizer.encode(sentence, return_tensors='pt')\n",
    "        ids_tensor = ids_tensor.to(self.device)\n",
    "        with torch.no_grad():\n",
    "            out = self.model(input_ids=ids_tensor)\n",
    "        hidden_states = out.hidden_states\n",
    "        last_four_layers = [hidden_states[i] for i in (-1, -2, -3, -4)]\n",
    "        sentence_embedding = torch.cat(tuple(last_four_layers), dim=0)\n",
    "        sentence_embedding = torch.mean(sentence_embedding, dim=0)\n",
    "        sentence_embedding = torch.mean(sentence_embedding, dim=0)\n",
    "        return sentence_embedding.cpu().numpy()\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        X = X.apply(self.embed_sentence)\n",
    "        X = pd.DataFrame(X.values.tolist())\n",
    "        return X\n",
    "\n",
    "prep_pipeline = Pipeline([\n",
    "    (\"select_features\", SelectFeatures(features_list=[\"Zweck\"])),\n",
    "    (\"clean_text\", CleanText()),\n",
    "    (\"spacy_lemmatizer\", SpacyLemmatizer()),\n",
    "    (\"text_lowercase\", Lowercase()),\n",
    "    (\"tfidf_vectorizer\", TfIdfVectorizer()),\n",
    "    # (\"bert_sentence_embeddings\", BertSentenceEmbeddigs()),\n",
    "])\n",
    "\n",
    "# Example\n",
    "prep_pipeline.fit(X, y)\n",
    "prep_pipeline.transform(X).head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(prep_pipeline,open(\"pretrained_models/random_forest/prep_pipeline.pkl\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters hypertuning and model selection with GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lisandro\\Desktop\\Projects\\case-lisandro\\venv\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'estimator': RandomForestClassifier(class_weight='balanced', n_estimators=50,\n",
      "                       random_state=42, verbose=1), 'estimator__class_weight': 'balanced', 'estimator__n_estimators': 50}\n",
      "0.302764736429617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  50 out of  50 | elapsed:    0.6s finished\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "\n",
    "\n",
    "def custom_scorer_macro_f1(y_true,y_pred):\n",
    "    macro_f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    return macro_f1\n",
    "\n",
    "scorer_macro_f1 = make_scorer(custom_scorer_macro_f1, greater_is_better=True)\n",
    "\n",
    "def custom_scorer_weighted_f1(y_true,y_pred):\n",
    "    weighted_f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    return weighted_f1\n",
    "\n",
    "scorer_weighted_f1 = make_scorer(custom_scorer_weighted_f1, greater_is_better=True)\n",
    "\n",
    "\n",
    "def execute_pipeline(features,labels, search_space=[\n",
    "                    {\"estimator\": [RandomForestClassifier(random_state=42, verbose=1)],\n",
    "                    \"estimator__n_estimators\": [10,50,100],\n",
    "                    # \"estimator__max_depth\": [2, 6],\n",
    "                    \"estimator__class_weight\": ['balanced',None]\n",
    "                    }],\n",
    "                    cv = 5,\n",
    "                    verbose = 1,\n",
    "                    n_jobs = os.cpu_count() - 2,\n",
    "                    scoring = scorer_macro_f1):\n",
    "                    # scoring = {\"macro_f1\": scorer_macro_f1,\"weighted_f1\": scorer_weighted_f1}):\n",
    "    \n",
    "    pipe = Pipeline([(\"preprocessing\", prep_pipeline),\n",
    "                    (\"estimator\", RandomForestClassifier())])\n",
    "    \n",
    "    gridsearch = GridSearchCV(pipe, search_space, scoring=scoring, cv=cv, verbose=verbose,n_jobs=n_jobs,refit=True)\n",
    "    best_model = gridsearch.fit(features, labels)\n",
    "    print(best_model.best_params_)\n",
    "    print(best_model.best_score_)\n",
    "    return best_model\n",
    "\n",
    "best_estimator = execute_pipeline(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train best model on whole data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  50 out of  50 | elapsed:    0.6s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(class_weight='balanced', n_estimators=50,\n",
       "                       random_state=42, verbose=1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = best_estimator.best_params_[\"estimator\"]\n",
    "best_model.fit(prep_pipeline.transform(X), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save best model for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(best_model,open(\"pretrained_models/random_forest/best_estimator.pkl\", \"wb\" ))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9ffdfb0697f174c80f5cf0b5f22ddc30cefc12684f1f3af5faba8742076bf488"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
